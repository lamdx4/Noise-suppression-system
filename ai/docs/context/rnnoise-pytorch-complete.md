# RNNoise PyTorch - Complete Training Guide

Comprehensive guide for training RNNoise models from scratch to ESP32 deployment.

---

## üìö Table of Contents

1. [Overview](#overview)
2. [Project Structure](#project-structure)
3. [Prerequisites](#prerequisites)
4. [Complete Workflow](#complete-workflow)
5. [C Tools (dump_features)](#c-tools)
6. [Training Details](#training-details)
7. [Export & Deployment](#export-deployment)
8. [Customization](#customization)
9. [Troubleshooting](#troubleshooting)

---

## Overview

RNNoise PyTorch l√† custom implementation extracted t·ª´ Mozilla RNNoise reference, organized cho production training v√† ESP32 deployment.

**Features:**

- ‚úÖ PyTorch-only (no TensorFlow)
- ‚úÖ Modular (model/dataset/loss separated)
- ‚úÖ Sparsification (850KB models)
- ‚úÖ Production training/export scripts
- ‚úÖ Based exactly on reference code

**Model Stats:**

- Parameters: 1.5M (dense) ‚Üí 750K (sparse)
- Size: 6MB ‚Üí 850KB (quantized + sparse)
- Latency: <10ms per frame
- Quality: PESQ 2.3-2.5

---

## Project Structure

```
ai/
‚îú‚îÄ‚îÄ rnnoise-pytorch/              # Custom training project
‚îÇ   ‚îú‚îÄ‚îÄ rnnoise/                  # Python package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py             # Architecture (from reference)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py           # Feature loader
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loss.py              # Loss functions
‚îÇ   ‚îú‚îÄ‚îÄ sparsification/          # Sparse training (from reference)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py             # Production training ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ export_to_c.py       # PyTorch ‚Üí C ‚≠ê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training_logger.py   # JSON logging
‚îÇ   ‚îú‚îÄ‚îÄ configs/default.yaml
‚îÇ   ‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ references/rnnoise/           # Reference implementation
‚îÇ   ‚îú‚îÄ‚îÄ dump_features            # Feature extractor (built)
‚îÇ   ‚îú‚îÄ‚îÄ src/                     # C source
‚îÇ   ‚îî‚îÄ‚îÄ torch/                   # PyTorch reference
‚îÇ
‚îî‚îÄ‚îÄ docs/context/                 # Documentation
    ‚îî‚îÄ‚îÄ rnnoise-pytorch-complete.md  # This file
```

---

## Prerequisites

### Hardware

- GPU: NVIDIA GTX 1060+ (4GB+ VRAM)
- RAM: 16GB minimum
- Storage: 100GB+ free

### Software

**Python:**

```bash
python --version  # 3.8+

cd ai/rnnoise-pytorch
pip install -r requirements.txt
# torch>=2.0.0, numpy, tqdm, pyyaml
```

**Build Tools (Linux/WSL):**

```bash
sudo apt-get install build-essential autoconf automake libtool sox
```

**Windows (MinGW/MSYS2):**

```bash
pacman -S base-devel mingw-w64-x86_64-toolchain
```

---

## Complete Workflow

### Stage 1: Build C Tools (One-Time)

```bash
cd ai/references/rnnoise

# Generate build scripts
./autogen.sh

# Configure
./configure

# Build
make

# Verify
./dump_features
# Should show: Usage: dump_features <speech> <noise> <fg_noise> <output> <count>
```

**Output:** `dump_features` executable ready to use

---

### Stage 2: Prepare Audio Data

**Requirements:**

- Format: 48kHz, 16-bit, mono, raw PCM (NO WAV header!)
- Clean speech: 2-3 hours (Vietnamese recommended)
- Background noise: 30-60 minutes
- Foreground noise: 30-60 minutes (can reuse background)

**Convert WAV to PCM:**

```bash
# Single file
sox input.wav -r 48000 -c 1 -b 16 -e signed-integer output.pcm

# Batch convert
for file in *.wav; do
    sox "$file" -r 48000 -c 1 -b 16 -e signed-integer "${file%.wav}.pcm"
done

# Concatenate multiple files
cat speech1.pcm speech2.pcm speech3.pcm > clean_speech.pcm
cat noise1.pcm noise2.pcm > background_noise.pcm
cp background_noise.pcm foreground_noise.pcm  # Can reuse
```

**Verify PCM format:**

```bash
# File size should be: duration(s) √ó 48000 √ó 2 bytes
# Example: 1 hour = 3600s √ó 48000 √ó 2 = 345,600,000 bytes (~345MB)
ls -lh clean_speech.pcm
```

---

### Stage 3: Generate Training Features

**What dump_features does:**

1. Random sampling from input files
2. Gain randomization (SNR: -40 to +45 dB)
3. Mixing: `noisy = speech + bg_noise + fg_noise`
4. Random filtering, clipping, quantization
5. Extract 42 features + compute ideal Wiener gains
6. Output binary .f32 file

**Basic usage:**

```bash
cd ai/references/rnnoise

./dump_features \
    ../../data/clean_speech.pcm \
    ../../data/background_noise.pcm \
    ../../data/foreground_noise.pcm \
    features_30k.f32 \
    30000

# Arguments:
# 1. Clean speech PCM
# 2. Background noise PCM
# 3. Foreground noise PCM
# 4. Output .f32 file
# 5. Number of sequences (30000 = ~30GB dataset)
```

**Expected:**

- Time: 1-3 hours
- Output size: ~11.5 GB (30K sequences)
- Format: [30000, 2000, 98] float32

**With reverberation (optional):**

```bash
# Create RIR list
ls /path/to/rirs/*.pcm > rir_list.txt

# Generate with RIR
./dump_features \
    -rir_list rir_list.txt \
    clean_speech.pcm \
    background_noise.pcm \
    foreground_noise.pcm \
    features_30k.f32 \
    30000
```

**Parallel generation (faster):**

```bash
# Edit script first
nano scripts/dump_features_parallel.sh

# Run (spawns 8 parallel processes)
./scripts/dump_features_parallel.sh \
    ./dump_features \
    ../../data/clean_speech.pcm \
    ../../data/background_noise.pcm \
    ../../data/foreground_noise.pcm \
    features_30k.f32 \
    30000
```

---

### Stage 4: Train Model

**Basic training:**

```bash
cd ../../rnnoise-pytorch

python scripts/train.py \
    ../references/rnnoise/features_30k.f32 \
    ./output \
    --epochs 150 \
    --batch-size 128
```

**RECOMMENDED - With sparsification:**

```bash
python scripts/train.py \
    ../references/rnnoise/features_30k.f32 \
    ./output_sparse \
    --sparse \
    --epochs 150 \
    --batch-size 128 \
    --gru-size 384 \
    --lr 1e-3 \
    --gamma 0.25 \
    --log-dir ../logs \
    --experiment-name rnnoise_vn_384
```

**Arguments:**

- `features`: Path to .f32 feature file
- `output`: Output directory for checkpoints
- `--sparse`: Enable sparsification (50% pruning)
- `--epochs`: Number of epochs (150 recommended)
- `--batch-size`: Batch size (128 default, reduce if OOM)
- `--gru-size`: GRU hidden size (384=best, 256=faster)
- `--log-dir`: JSON logging directory (optional)

**Expected:**

- Time: 4-8 hours (GPU), 2-3 days (CPU)
- Final loss: ~0.01
- Checkpoints: Saved every epoch in `output/checkpoints/`

**Monitor training:**

```bash
# Progress bar shows real-time loss
# JSON logs (if enabled) in ../logs/

# Check latest checkpoint
ls -lt output/checkpoints/ | head
```

---

### Stage 5: Export to C

```bash
python scripts/export_to_c.py \
    --quantize \
    ./output_sparse/checkpoints/rnnoise_150.pth \
    ./exported
```

**Arguments:**

- `weightfile`: Path to .pth checkpoint
- `export_folder`: Output directory
- `--quantize`: Convert float32 ‚Üí int8 (recommended)
- `--struct-name`: C struct name (default: RNNoise)

**Output:**

```
exported/
‚îú‚îÄ‚îÄ rnnoise_data.c  # Model weights
‚îî‚îÄ‚îÄ rnnoise_data.h  # Header file
```

**Sizes:**

- Without quantization: 6MB
- With quantization: 1.5MB
- With quantization + sparse: 850KB ‚≠ê

---

### Stage 6: Build & Test

```bash
cd ../references/rnnoise

# Copy exported files
cp ../../rnnoise-pytorch/exported/rnnoise_data.c src/
cp ../../rnnoise-pytorch/exported/rnnoise_data.h src/

# Rebuild RNNoise library
make clean
make

# Test with demo
./examples/rnnoise_demo \
    ../../data/noisy_test.pcm \
    ../../data/clean_output.pcm

# Convert output to WAV to listen
sox -r 48000 -c 1 -b 16 -e signed-integer \
    ../../data/clean_output.pcm \
    ../../data/clean_output.wav

# Play (Linux)
aplay ../../data/clean_output.wav
```

---

### Stage 7: Deploy to ESP32

**Copy C files:**

```bash
cp src/rnnoise_data.c /path/to/arduino/project/
cp src/rnnoise_data.h /path/to/arduino/project/
```

**Include in Arduino:**

```c
#include "rnnoise.h"

DenoiseState *st;
float input[FRAME_SIZE];
float output[FRAME_SIZE];

void setup() {
    st = rnnoise_create(NULL);
}

void loop() {
    // Get audio frame (480 samples @ 48kHz = 10ms)
    // ... fill input[] ...

    // Denoise
    rnnoise_process_frame(st, output, input);

    // Output processed audio
    // ... use output[] ...
}
```

**ESP32 Requirements:**

- Device: ESP32-S3 with PSRAM (8MB recommended)
- Flash: 850KB for model
- RAM: ~800KB during inference
- Processing: ~5-8ms per 10ms frame (real-time capable)

---

## C Tools Deep Dive

### dump_features Internals

**Input Processing:**

1. Random chunk selection (1 second each)
2. Gain randomization:
   - Speech: -45 to 0 dB
   - BG noise: -30 to +10 dB
   - FG noise: -30 to +10 dB
3. Mixing with random filtering
4. Optional RIR convolution
5. Random clipping/quantization

**Feature Extraction:**

- 42 hand-crafted features:
  - Bark-scale bands (22)
  - Pitch correlation (6)
  - Spectral differences (14)
- Ideal Wiener gains (22 bands)
- VAD target (binary)

**Output Format:**

```
Binary .f32 file (little-endian)
Shape: [num_sequences, 2000, 98]
  - num_sequences: User specified
  - 2000: Frames per sequence (20 seconds @ 10ms frames)
  - 98: Features per frame
    - [0:64]: Input features (42 actual + padding)
    - [65:96]: Target gains (32 values, 22 used)
    - [97]: VAD target
```

### Manual Build (if autotools fails)

```bash
cd ai/references/rnnoise/src

gcc -o dump_features \
    dump_features.c \
    denoise.c \
    pitch.c \
    celt_lpc.c \
    kiss_fft.c \
    parse_lpcnet_weights.c \
    rnnoise_tables.c \
    -I. \
    -DTRAINING \
    -lm \
    -O3

# Move to convenient location
mv dump_features ../
```

---

## Training Details

### Loss Function

**Perceptual Gain Loss:**

```python
# From reference (exact formula)
target_gain = torch.clamp(gain, min=0)
target_gain = target_gain * (torch.tanh(8*target_gain)**2)

e = pred_gain**gamma - target_gain**gamma
gain_loss = torch.mean((1+5.*vad)*mask(gain)*(e**2))
```

- `gamma=0.25`: Perceptual weighting (penalizes low gain errors more)
- `mask(gain)`: Ignore invalid targets (-1 values)
- `vad` weighting: Speech frames 6√ó more important

**VAD Loss:**

```python
vad_loss = torch.mean(
    torch.abs(2*vad-1) * (
        -vad*torch.log(.01+pred_vad) -
        (1-vad)*torch.log(1.01-pred_vad)
    )
)
```

- Binary cross-entropy
- Confidence weighting (high at 0/1, low at 0.5)
- Weight: 0.001 (auxiliary task)

**Combined:**

```python
loss = gain_loss + 0.001 * vad_loss
```

### Optimizer

```python
# From reference (exact settings)
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-3,
    betas=[0.8, 0.98],  # Custom betas!
    eps=1e-8
)
```

### Learning Rate Schedule

```python
# From reference (exact formula)
scheduler = torch.optim.lr_scheduler.LambdaLR(
    optimizer,
    lr_lambda=lambda x: 1 / (1 + 5e-5 * x)
)
# Decays gradually: 1e-3 ‚Üí 5e-4 over 100K steps
```

### Sparsification

**Progressive pruning (if --sparse enabled):**

- Start: Step 6000
- End: Step 20000
- Interval: Every 100 steps
- Interpolation: Cubic (exponent=3)

**Per-gate targets:**

```yaml
W_hr: 0.3 # Recurrent reset
W_hz: 0.2 # Recurrent update (least aggressive)
W_hn: 0.5 # Recurrent new (most aggressive)
W_ir: 0.3 # Input reset
W_iz: 0.2 # Input update
W_in: 0.5 # Input new
```

**Block structure:** 8√ó4 (hardware-friendly)

---

## Export & Deployment

### Export Process

1. **Load checkpoint:**

```python
checkpoint = torch.load('rnnoise_150.pth')
model = RNNoise(**checkpoint['model_kwargs'])
model.load_state_dict(checkpoint['state_dict'])
```

2. **Remove weight normalization:**

```python
def _remove_weight_norm(m):
    torch.nn.utils.remove_weight_norm(m)
model.apply(_remove_weight_norm)
```

3. **Export each layer:**

```python
# Uses reference's weight-exchange library
for name, module in model.named_modules():
    if isinstance(module, nn.Linear):
        wexchange.torch.dump_torch_dense_weights(...)
    elif isinstance(module, nn.Conv1d):
        wexchange.torch.dump_torch_conv1d_weights(...)
    elif isinstance(module, nn.GRU):
        wexchange.torch.dump_torch_gru_weights(
            ..., input_sparse=True, recurrent_sparse=True
        )
```

4. **Quantization (if --quantize):**

- float32 ‚Üí int8 for most layers
- Excluded: conv1, dense_out, vad_dense
- Scale: 1/128 for unquantized

### Deployment Options

**Option 1: Static linking (recommended)**

```c
// Include in firmware
#include "rnnoise_data.h"

DenoiseState *st = rnnoise_create(NULL);  // Uses built-in model
```

**Option 2: Dynamic loading**

```c
// Load from file
RNNModel *model = rnnoise_model_from_file("weights.bin");
DenoiseState *st = rnnoise_create(model);
```

---

## Customization

### Model Size

**In training:**

```bash
# Smaller (faster inference, less quality)
python scripts/train.py ... --gru-size 256

# Standard (balanced)
python scripts/train.py ... --gru-size 384  # Default

# Larger (slower, better quality)
python scripts/train.py ... --gru-size 512
```

**Results:**
| GRU Size | Params | Size (sparse+q8) | Quality | Speed |
|----------|--------|------------------|---------|-------|
| 256 | 900K | 600 KB | PESQ 2.2| 3ms |
| 384 | 1.5M | 850 KB | PESQ 2.4| 5ms |
| 512 | 2.6M | 1.2 MB | PESQ 2.5| 8ms |

### Loss Tuning

**Edit `rnnoise/loss.py`:**

```python
# Change perceptual gamma
def perceptual_gain_loss(..., gamma=0.20):  # Lower = more penalty on low gains
    ...

# Change VAD weight
loss = gain_loss + 0.002 * vad_loss  # Higher = better VAD
```

### Sparsity Targets

**Edit `configs/default.yaml`:**

```yaml
sparsification:
  targets:
    W_hn: 0.7 # More aggressive (70% pruned)
    W_hz: 0.3 # Less conservative
```

**Trade-off:**

- Higher sparsity ‚Üí Smaller model, faster inference
- Lower sparsity ‚Üí Better quality

---

## Troubleshooting

### Build Issues

**dump_features build fails:**

```bash
# Install autotools
sudo apt-get install autoconf automake libtool

# Or use manual compilation (see C Tools section)
```

**Windows build:**

```bash
# Use MSYS2
pacman -S base-devel mingw-w64-x86_64-toolchain

# Then follow Linux steps
```

### Training Issues

**CUDA out of memory:**

```bash
python scripts/train.py ... --batch-size 64  # Reduce from 128
```

**Training too slow:**

```bash
# Verify GPU usage
nvidia-smi

# Install CUDA-enabled PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Loss not decreasing:**

- Check dataset quality (valid PCM format?)
- Verify features.f32 not corrupted
- Try smaller model (--gru-size 256)

### Export Issues

**weight-exchange not found:**

```bash
# Verify reference folder intact
ls ai/references/rnnoise/torch/weight-exchange

# If missing, re-clone reference
```

**Export script fails:**

```python
# Check checkpoint format
checkpoint = torch.load('model.pth')
print(checkpoint.keys())  # Should have: state_dict, model_kwargs
```

### Deployment Issues

**ESP32 out of memory:**

- Use ESP32-S3 with 8MB PSRAM
- Enable PSRAM in Arduino IDE
- Reduce model size (gru-size 256)

**Inference too slow:**

- Enable compiler optimizations (-O3)
- Use ESP32-S3 (faster than ESP32)
- Check clock speed (240MHz recommended)

---

## Quick Reference

### Complete Command Sequence

```bash
# 1. Build tools (one-time)
cd ai/references/rnnoise && ./autogen.sh && ./configure && make

# 2. Convert audio
sox speech.wav -r 48000 -c 1 -b 16 speech.pcm
sox noise.wav -r 48000 -c 1 -b 16 noise.pcm

# 3. Generate features
./dump_features speech.pcm noise.pcm noise.pcm features_30k.f32 30000

# 4. Train (sparse)
cd ../../rnnoise-pytorch
python scripts/train.py ../references/rnnoise/features_30k.f32 ./output --sparse --epochs 150

# 5. Export (quantized)
python scripts/export_to_c.py --quantize ./output/checkpoints/rnnoise_150.pth ./exported

# 6. Build & test
cd ../references/rnnoise
cp ../../rnnoise-pytorch/exported/* src/
make clean && make
./examples/rnnoise_demo test_noisy.pcm test_clean.pcm

# 7. Deploy to ESP32
cp src/rnnoise_data.* /path/to/arduino/project/
```

### File Locations

```
ai/
‚îú‚îÄ‚îÄ rnnoise-pytorch/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/train.py          # Training script
‚îÇ   ‚îú‚îÄ‚îÄ scripts/export_to_c.py    # Export script
‚îÇ   ‚îî‚îÄ‚îÄ rnnoise/                  # Python package
‚îÇ
‚îú‚îÄ‚îÄ references/rnnoise/
‚îÇ   ‚îú‚îÄ‚îÄ dump_features             # Feature extractor
‚îÇ   ‚îú‚îÄ‚îÄ features_30k.f32          # Generated features
‚îÇ   ‚îî‚îÄ‚îÄ src/rnnoise_data.c        # Exported model
‚îÇ
‚îî‚îÄ‚îÄ docs/context/
    ‚îî‚îÄ‚îÄ rnnoise-pytorch-complete.md  # This guide
```

---

## Additional Resources

- **Paper:** [A Hybrid DSP/Deep Learning Approach](https://jmvalin.ca/papers/rnnoise_mmsp2018.pdf)
- **Reference Repo:** https://github.com/xiph/rnnoise
- **DNS Challenge:** https://github.com/microsoft/DNS-Challenge
- **Datasets:** See `references/rnnoise/datasets.txt`

---

**Total workflow time:** 2-3 days (mostly waiting for training)

**Result:** Production-ready Vietnamese speech enhancement for ESP32! üáªüá≥üéØ
