# RNNoise Preprocessing & Model Architecture

Hi·ªÉu c√°ch RNNoise bi·∫øn audio th√†nh features v√† architecture c·ªßa neural network.

---

## Big Picture

**Training flow:**

```
Raw Audio (noisy + clean)
    ‚Üì Preprocessing (dump_features)
42 Features + Ground Truth Gains
    ‚Üì Training (PyTorch)
GRU Model Weights
    ‚Üì Export
C Code cho Inference
```

Ph·∫ßn n√†y gi·∫£i th√≠ch 2 b∆∞·ªõc ƒë·∫ßu: **Preprocessing** v√† **Model Architecture**.

---

# PH·∫¶N 1: PREPROCESSING (dump_features)

## Nhi·ªám V·ª• Ch√≠nh

T·∫°o training pairs t·ª´ audio th√¥:

```
Input: 3 files (clean speech, bg noise, fg noise)
Output: File .f32 ch·ª©a [42 features, 22 gains, 1 VAD] √ó 30,000
```

---

## B∆∞·ªõc 1: Synthetic Mixing

### **Random Picking**

M·ªói sequence (1 gi√¢y):

- Random pick 1s t·ª´ clean speech
- Random pick 1s t·ª´ background noise
- Random pick 1s t·ª´ foreground noise

**T·∫°i sao random:** T·∫°o diversity, tr√°nh model h·ªçc "v·ªã tr√≠" thay v√¨ "pattern"

### **Random Gains (SNR Levels)**

```
Speech gain: 10^(random(-45 to 0 dB) / 20)
BG noise gain: 10^(random(-30 to +10 dB) / 20)
FG noise gain: 10^(random(-30 to +10 dB) / 20)
```

**√ù nghƒ©a:**

- Speech: -45 to 0 dB = t·ª´ th√¨ th·∫ßm ƒë·∫øn n√≥i to
- Noise: -30 to +10 dB = t·ª´ c·ª±c nh·ªè ƒë·∫øn √°t c·∫£ speech
- **SNR range:** -40 to +45 dB (c·ª±c k·ª≥ diverse!)

**T·∫°i sao random SNR:**

- Real-world c√≥ m·ªçi ƒëi·ªÅu ki·ªán
- Model ph·∫£i robust v·ªõi m·ªçi noise levels

### **Mixing Formula**

```
Noisy = Speech √ó speech_gain
      + BG_noise √ó bg_gain
      + FG_noise √ó fg_gain (87.5% chance)
```

**87.5% foreground:** Kh√¥ng ph·∫£i l√∫c n√†o c≈©ng c√≥ transient noise

---

## B∆∞·ªõc 2: Data Augmentation

Kh√¥ng ch·ªâ mixing ƒë∆°n gi·∫£n! C√≥ th√™m **5 augmentations**:

### **2a. Random Filtering (Spectral Coloring)**

```
Biquad IIR filters v·ªõi random coefficients:
- C·∫£ speech, bg noise, fg noise ƒë·ªÅu qua filter kh√°c nhau
- M√¥ ph·ªèng: phone quality, room acoustics, device characteristics
```

**T·∫°i sao:** Microphone/speaker kh√°c nhau = EQ kh√°c nhau

### **2b. Random Start Position**

```
75% samples: Start t·ª´ ƒë·∫ßu file
25% samples: Start t·ª´ random position (exponential distribution)
```

**T·∫°i sao:** Kh√¥ng c√≥ "beginning bias"

### **2c. Reverberation (Optional)**

N·∫øu c√≥ RIR dataset:

```
50% samples: Apply room impulse response
- Speech: Early reflections only (first 10ms)
- Noisy: Full reverb
```

**T·∫°i sao:**

- Clean speech kh√¥ng c√≥ late reflections (kh√¥ng echo)
- Noisy c√≥ full reverb (realistic)
- Model h·ªçc suppress reverb tail

### **2d. Input Clipping**

```
25% samples: Clip to ¬±32767 (0 dBFS)
```

**T·∫°i sao:** Real-world c√≥ clipping (ADC saturation), model c·∫ßn handle

### **2e. Quantization**

```
50% samples: Round to 16-bit integers
```

**T·∫°i sao:** Real audio = 16-bit, kh√¥ng ph·∫£i float32

---

## B∆∞·ªõc 3: Feature Extraction (Same as Inference)

V·ªõi **noisy audio**, extract 42 features:

- 22 Bark band energies
- 6 Spectral correlations
- 7 Delta features
- 7 Pitch features

(Chi ti·∫øt xem `rnnoise-inference-flow.md`)

**Key:** Features t·ª´ **noisy audio**, kh√¥ng ph·∫£i clean!

---

## B∆∞·ªõc 4: Ground Truth Computation

### **Ideal Wiener Gain**

T·ª´ clean v√† noisy, compute ideal gains:

```
For each Bark band i:
    clean_energy = compute_energy(clean_spectrum, band[i])
    noisy_energy = compute_energy(noisy_spectrum, band[i])

    ideal_gain[i] = sqrt(clean_energy / noisy_energy)

    # Cap at 1.0 (never amplify)
    if ideal_gain[i] > 1.0:
        ideal_gain[i] = 1.0

    # Mark invalid if too quiet
    if clean_energy < threshold or noisy_energy < threshold:
        ideal_gain[i] = -1  # Mask out
```

**C√¥ng th·ª©c Wiener filter:** Optimal gain trong MSE sense

**Cap at 1.0:** Kh√¥ng amplify (ch·ªâ suppress)

**Masking:** -1 = "don't learn from this band" (too quiet/unreliable)

### **VAD Ground Truth**

```
Viterbi algorithm tr√™n energy:
- Segment th√†nh speech/silence
- Smooth v·ªõi HMM (tr√°nh flickering)
- Output: 0 (silence) or 1 (speech)
```

**T·∫°i sao Viterbi:** Tr√°nh VAD nh·∫£y li√™n t·ª•c (temporal consistency)

---

## B∆∞·ªõc 5: Save Training Pair

```
ÊØè sequence l∆∞u:
[42 features] [22 ideal_gains] [1 VAD] = 65 float32 values
       ‚Üë Input         ‚Üë Target      ‚Üë Auxiliary
```

Repeat 30,000 l·∫ßn ‚Üí File ~8-12 GB

---

## Nh·ªØng ƒêi·ªÉm Hay Ho (Preprocessing)

### 1. **Synthetic Data > Real Data**

**T·∫°i sao kh√¥ng d√πng real noisy recordings?**

‚ùå **Real recordings:**

- Kh√¥ng c√≥ ground truth (kh√¥ng bi·∫øt clean n√†o)
- √çt diversity (ch·ªâ v√†i scenarios)
- Expensive (c·∫ßn ng∆∞·ªùi thu √¢m)

‚úÖ **Synthetic mixing:**

- Perfect ground truth (c√≥ clean + noise ri√™ng)
- Infinite diversity (random combinations)
- Free (code t·ª± generate)

**Trade-off:** Ph·∫£i careful v·ªõi augmentation ƒë·ªÉ realistic

---

### 2. **Wiener Filter = Optimal Baseline**

```
Wiener gain = Best possible gain (statistically)

N·∫øu model predict g·∫ßn Wiener ‚Üí Excellent!
```

Model kh√¥ng c·∫ßn "invent" better method, ch·ªâ c·∫ßn **learn to approximate Wiener filter**.

**Brilliant:** Target kh√¥ng ph·∫£i "perfect clean" m√† l√† "statistically optimal gains"

---

### 3. **Masking Invalid Bands**

```
gain = -1 ‚Üí Loss = 0 (kh√¥ng h·ªçc t·ª´ band n√†y)

T·∫°i sao:
- Band qu√° y√™n tƒ©nh ‚Üí noise floor, unreliable
- Silence frames ‚Üí kh√¥ng c√≥ speech ƒë·ªÉ h·ªçc
- Target speaker not active ‚Üí avoid learning noise
```

Smart masking = better training signal

---

### 4. **Multi-Scale SNR**

```
SNR range: -40 to +45 dB = 85 dB dynamic range!

Covers:
- Whisper in quiet room (+40 dB)
- Normal speech with AC (-5 dB)
- Shouting in construction site (-30 dB)
```

Model ph·∫£i universal ‚Üí train v·ªõi extreme conditions

---

### 5. **Foreground Noise Strategy**

```
12.5% samples: No foreground
87.5% samples: With foreground (random bursts)

T·∫°i sao kh√¥ng 100%?
- Real-world: Transients kh√¥ng li√™n t·ª•c
- Model learn: "Sometimes c√≥, sometimes kh√¥ng"
- Tr√°nh bias: "Lu√¥n expect transient"
```

**Subtle detail = big impact** on generalization

---

# PH·∫¶N 2: MODEL ARCHITECTURE

## GRU Network Design

**Philosophy:** Small model, big context

```
Input: 42 features (1 frame = 10ms)
Output: 22 gains + 1 VAD
Hidden: Persistent state (temporal memory)
```

---

## Layer-by-Layer Breakdown

### **Input Processing**

```
Input: [Batch, Sequence, 42]
       ‚Üì
Dense(42 ‚Üí 128, tanh) - "Feature compression"
       ‚Üì
Conv1D(128 ‚Üí 96, kernel=3, tanh) - "Temporal smoothing"
```

**Dense layer:** Combine raw features  
**Conv1D:** Look at 3-frame window (30ms context)

**Output:** [Batch, Sequence, 96]

---

### **GRU Stack (Core)**

```
GRU1: 96 ‚Üí 384 units
       ‚Üì (carry hidden state)
GRU2: 384 ‚Üí 384 units
       ‚Üì (carry hidden state)
GRU3: 384 ‚Üí 384 units
       ‚Üì (carry hidden state)
```

**T·∫°i sao 3 layers?**

- Layer 1: Low-level patterns (phonemes)
- Layer 2: Mid-level patterns (words)
- Layer 3: High-level patterns (sentences)

**T·∫°i sao 384 units?**

- Power of 2 friendly (SIMD optimization)
- Sweet spot (256 = underfitting, 512 = overkill)
- Tested empirically

---

### **Multi-Scale Fusion**

```
conv2_out: [Batch, Seq, 96]
gru1_out:  [Batch, Seq, 384]
gru2_out:  [Batch, Seq, 384]
gru3_out:  [Batch, Seq, 384]

Concatenate all:
fused = [Batch, Seq, 1248]  (96+384+384+384)
```

**T·∫°i sao concatenate t·∫•t c·∫£?**

- GRU1: Short-term context
- GRU2: Medium-term context
- GRU3: Long-term context
- Conv: Local features

**All scales matter!** ‚Üí Combine c·∫£ 4

---

### **Output Layers**

```
fused [1248]
    ‚Üì
Dense(1248 ‚Üí 22, sigmoid) ‚Üí gains [0-1]
    ‚Üì
Dense(1248 ‚Üí 1, sigmoid) ‚Üí VAD [0-1]
```

**Sigmoid activation:** Bound output to [0,1]

**Post-processing (during inference):**

```
# Clip gains to [0.6, 1.0]
gains = 0.6 + 0.4 * sigmoid_output
```

Never suppress below 60%!

---

## Training Strategy

### **Loss Function (PyTorch version)**

**Gain Loss (Perceptual):**

```python
# Gamma = 0.25 (perceptual exponent)
error = predicted_gain^0.25 - target_gain^0.25

# Weight by VAD (speech present = more important)
weighted_error = (1 + 5*VAD) √ó mask √ó error¬≤

gain_loss = mean(weighted_error)
```

**T·∫°i sao gamma=0.25?**

- Linear MSE: treat all errors equally
- Power 0.25: penalize errors ·ªü low gains nhi·ªÅu h∆°n
- **Perceptual:** Tai ng∆∞·ªùi nh·∫°y c·∫£m v·ªõi small gains h∆°n large gains

**T·∫°i sao weight by VAD?**

- Speech frames: 6√ó more important
- Silence frames: Still learn (don't ignore noise-only)

**VAD Loss (Binary Cross-Entropy):**

```python
# Weight by confidence
weight = |2*VAD - 1|  # 1 at extremes, 0 at 0.5

vad_loss = mean(weight √ó BCE(predicted_VAD, target_VAD))
```

**Total Loss:**

```python
loss = gain_loss + 0.001 √ó vad_loss
       ‚Üë Main      ‚Üë Auxiliary (1000√ó smaller)
```

VAD = helper task, kh√¥ng ph·∫£i primary objective

---

### **Optimizer & Schedule**

```
Optimizer: AdamW
- Beta: [0.8, 0.98] (faster momentum decay)
- Epsilon: 1e-8
- LR: 1e-3 (initial)

LR Schedule: Lambda decay
LR(step) = 1 / (1 + 5e-5 √ó step)

Batch size: 128
Sequence length: 2000 frames (20 seconds)
```

**T·∫°i sao sequence 2000?**

- GRU c·∫ßn long context ƒë·ªÉ h·ªçc temporal patterns
- 20s = enough ƒë·ªÉ cover multi-syllable words/phrases

---

### **Regularization**

**Weight Constraints:**

```python
# Clip all weights to [-0.499, 0.499]
W = clip(W, -0.499, 0.499)
```

**T·∫°i sao 0.499?**

- Prevents exploding gradients
- Keep weights quantization-friendly (int8 conversion)
- Empirically stable

**L2 Regularization:**

```python
L2 penalty: 1e-6
```

Very small (just prevents extreme outliers)

---

## Sparsification (Advanced)

### **Progressive Pruning**

```
Step 0-6000: Dense training
    ‚Üì Model learns with full capacity

Step 6000-20000: Gradual pruning
    progress = (step - 6000) / 14000
    sparsity = target_sparsity √ó progress¬≥

    Every 100 steps:
        - Find smallest |weights|
        - Zero out bottom X%
        - Continue training

Step 20000+: Fixed sparsity fine-tuning
    ‚Üì Model adapts to sparse structure
```

**T·∫°i sao cubic progress (¬≥)?**

- Slow start (careful pruning)
- Fast end (aggressive cleanup)
- Smooth transition

### **Block Sparsity**

```
Instead of: Random individual zeros
Use: 8√ó4 block zeros

Example:
[0 0 0 0  x x x x  0 0 0 0 ...]
 ‚îî block 1‚îò‚îîblock 2‚îò‚îîblock 3‚îò

Hardware can skip entire blocks ‚Üí Faster!
```

**Magic numbers:**

- W_hn: 50% sparsity (most aggressive)
- W_hz: 20% sparsity (least aggressive)
- W_hr: 30% sparsity (middle ground)

Different layers = different importance!

---

## Nh·ªØng ƒêi·ªÉm Hay Ho (Model)

### 1. **Multi-Scale Architecture**

```
Conv: Local (3 frames = 30ms)
GRU1: Short (phonemes ~50ms)
GRU2: Medium (words ~200ms)
GRU3: Long (phrases ~500ms)

All fused ‚Üí Comprehensive understanding!
```

Kh√¥ng c√≥ layer n√†o "useless", each captures different timescale.

---

### 2. **Tiny Model, Big Memory**

```
Parameters: 1.5M (PyTorch) or 85K (Keras)
Memory footprint: 30KB + model

But: Infinite temporal context via GRU hidden state!
```

**Recurrence = memory efficiency**

Contrast with U-Net:

- U-Net: 450K params, finite window (1s)
- RNNoise: 85K params, infinite memory

**GRU magic!**

---

### 3. **Auxiliary VAD Task**

```
Main: Predict gains (denoising)
Aux: Predict VAD (speech detection)

Why both?
- VAD helps denoise (know when to suppress)
- Denoise helps VAD (clean features easier to classify)
- Multi-task learning = better representations
```

**Synergy:** Two tasks improve each other

---

### 4. **Perceptual Loss (Gamma=0.25)**

```
Linear loss:    Equal penalty cho m·ªçi errors
Perceptual:     Penalize errors theo tai ng∆∞·ªùi

Example:
Gain = 0.2:  delta ¬±0.1 ‚Üí VERY noticeable
Gain = 0.9:  delta ¬±0.1 ‚Üí Barely noticeable

Gamma = 0.25 captures this!
```

**Loss design = critical** cho quality

---

### 5. **Block Sparsity = Hardware-Aware**

```
Random sparsity: Theoretical speedup
[x 0 x 0 x 0 ...] ‚Üí Hard to optimize

Block sparsity: Practical speedup
[x x x x 0 0 0 0 ...] ‚Üí Skip whole blocks!

ESP32 SIMD: Process 4-8 values at once
‚Üí Block size 8√ó4 perfect!
```

**ML for embedded = must think hardware!**

---

## Training Timeline

```
Feature generation: 1-3 gi·ªù (30K sequences)
    ‚Üì
PyTorch training: 4-8 gi·ªù (150 epochs, GPU)
    ‚Üì Checkpoints every epoch
Export to C: <5 ph√∫t
    ‚Üì
Ready for deployment!

Total: ~6-11 gi·ªù end-to-end
```

---

## Takeaways

### **Preprocessing Insights:**

1. **Synthetic > Real** (with careful augmentation)
2. **Wiener filter = optimal target** (statistically)
3. **Extreme diversity** (SNR -40 to +45 dB)
4. **Smart masking** (invalid bands = don't learn)

### **Model Insights:**

1. **Multi-scale fusion** (local + short + medium + long)
2. **GRU = infinite memory** (stateful processing)
3. **Perceptual loss** (gamma=0.25 matches human hearing)
4. **Block sparsity** (hardware-aware pruning)
5. **Auxiliary VAD** (multi-task synergy)

**Engineering excellence** trong m·ªói detail! üéØ
